<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.46" />


<title>Machine Learning - Predictions with linear regressions - Michael Fuchs</title>
<meta property="og:title" content="Machine Learning - Predictions with linear regressions - Michael Fuchs">



  








<link href='//cdn.bootcss.com/highlight.js/9.11.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/MFuchs.png"
         width="50"
         height="50"
         alt="MFuchs">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/MFuchs1989/Bdown">GitHub</a></li>
    
    <li><a href="https://www.linkedin.com/in/michael-fuchs-139172131/">LinkedIn</a></li>
    
    <li><a href="https://twitter.com/Stat_Michael">Twitter</a></li>
    
    <li><a href="https://www.xing.com/profile/Michael_Fuchs426/cv?sc_o=mxb_p">XING</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">20 min read</span>
    

    <h1 class="article-title">Machine Learning - Predictions with linear regressions</h1>

    
    <span class="article-date">2019/01/11</span>
    

    <div class="article-content">
      <pre class="r"><code>library(tidyverse)
library(caret)
library(car)
library(glmnet)</code></pre>
<pre class="r"><code>machine &lt;- read_csv(&quot;machine.csv&quot;)
cars &lt;- read_csv(&quot;cars.csv&quot;)</code></pre>
<div id="table-of-content" class="section level1">
<h1>Table of Content</h1>
<ul>
<li>1 Introduction</li>
<li>2 Presentation of the data records used</li>
<li>3 Dividing the data into a training part and a test part</li>
<li>4 Removing problematic features</li>
<li>4.1 Machine dataset</li>
<li>4.2 Cars dataset</li>
<li>5 Assessing linear regression models</li>
<li>6 How to check some summary outputs individually</li>
<li>6.1 Residual analysis</li>
<li>6.2 Significance tests for linear regression</li>
<li>6.3 Residual standard error (RSE)</li>
<li>6.4 R<sup>2</sup></li>
<li>7 Test set performance with the MSE</li>
<li>8 Problems with linear regression</li>
<li>8.1 Multicollinearity</li>
<li>8.2 Outliers</li>
<li>8.2.1 Compare RSE</li>
<li>8.2.2 Compare MSE</li>
<li>9 Future selection</li>
<li>9.1 Forward selection</li>
<li>9.2 Backward selection</li>
<li>9.3 Comparing the calculated MSE</li>
<li>10 Regularization</li>
<li>11 Conclusion</li>
</ul>
</div>
<div id="introduction" class="section level1">
<h1>1 Introduction</h1>
<p>I have already written several general contributions on linear regression models (<a href="https://michael-fuchs.netlify.com/2018/09/21/regression-analysis/">“here”</a> and <a href="https://michael-fuchs.netlify.com/2018/10/02/special-regression-analysis/">“here”</a>). Likewise, I have already taken up the issue of how linear regression models can be trained and tested (<a href="https://michael-fuchs.netlify.com/2018/12/18/machine-learning-training-and-testing-sets-regression-modeling/">“Machine Learning - Training and Testing Sets: Regression Modeling”</a>) and how unimportant variables can be identified (<a href="https://michael-fuchs.netlify.com/2018/12/25/machine-learning-regression-regularization/">“Machine Learning - Regression Regularization”</a>).</p>
<p>In this publication, the prediction of the dependent variables from two different data networks should be central. In particular, the potential for improvement of the predictive power of the created models shall be considered.</p>
<p>For this post the dataset <em>machine</em> from the UCI- Machine Learning Repository platform <a href="https://archive.ics.uci.edu/ml/datasets.html">“UCI”</a> was used as well as the dataset <em>cars</em> from the caret-package. A copy of the records is available at <a href="https://drive.google.com/open?id=1tMtpU5xEkijF-GceVEmGwRQCVrhWNrvf" class="uri">https://drive.google.com/open?id=1tMtpU5xEkijF-GceVEmGwRQCVrhWNrvf</a> (machine) and <a href="https://drive.google.com/open?id=1E_NWwWEBBby456SuHt3qxHrsDpTzcUuv" class="uri">https://drive.google.com/open?id=1E_NWwWEBBby456SuHt3qxHrsDpTzcUuv</a> (cars).</p>
</div>
<div id="presentation-of-the-data-records-used" class="section level1">
<h1>2 Presentation of the data records used</h1>
<p>The first dataframe (machine) contains the characteristics of different CPU models, such as the cycle time and the amount cache memory. PRP will be the dependent variable.The variables that are superfluous for this study are excluded in advance.</p>
<pre class="r"><code>machine &lt;- machine[, 4:10]
head(machine, n = 5)</code></pre>
<pre><code>## # A tibble: 5 x 7
##    MYCT  MMIN  MMAX  CACH CHMIN CHMAX   PRP
##   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;
## 1   125   256  6000   256    16   128   198
## 2    29  8000 32000    32     8    32   269
## 3    29  8000 32000    32     8    32   220
## 4    29  8000 32000    32     8    32   172
## 5    29  8000 16000    32     8    16   132</code></pre>
<p>The second datafame (cars) contains details about different used car components, such as the number of cylinders or the number of miles the car has been driven.</p>
<pre class="r"><code>cars &lt;- cars[, -1]
glimpse(cars)</code></pre>
<pre><code>## Observations: 804
## Variables: 18
## $ Price       &lt;dbl&gt; 22661.05, 21725.01, 29142.71, 30731.94, 33358.77, ...
## $ Mileage     &lt;int&gt; 20105, 13457, 31655, 22479, 17590, 23635, 17381, 2...
## $ Cylinder    &lt;int&gt; 6, 6, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,...
## $ Doors       &lt;int&gt; 4, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4,...
## $ Cruise      &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...
## $ Sound       &lt;int&gt; 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,...
## $ Leather     &lt;int&gt; 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,...
## $ Buick       &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ Cadillac    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ Chevy       &lt;int&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ Pontiac     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ Saab        &lt;int&gt; 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...
## $ Saturn      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ convertible &lt;int&gt; 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ coupe       &lt;int&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ hatchback   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ sedan       &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,...
## $ wagon       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</code></pre>
</div>
<div id="dividing-the-data-into-a-training-part-and-a-test-part" class="section level1">
<h1>3 Dividing the data into a training part and a test part</h1>
<pre class="r"><code>machine_sampling_vector &lt;- createDataPartition(machine$PRP, p = 0.85, list = FALSE)
machine_train &lt;- machine[machine_sampling_vector,]
machine_train_labels &lt;- machine$PRP[machine_sampling_vector]
machine_test &lt;- machine[-machine_sampling_vector,]
machine_test_labels &lt;- machine$PRP[-machine_sampling_vector]
machine_train_features &lt;- machine[, 1:6]</code></pre>
<pre class="r"><code>cars_sampling_vector &lt;- createDataPartition(cars$Price, p = 0.85, list = FALSE)
cars_train &lt;- cars[cars_sampling_vector,]
cars_train_labels &lt;- cars$Price[cars_sampling_vector]
cars_test &lt;- cars[-cars_sampling_vector,]
cars_test_labels &lt;- cars$Price[-cars_sampling_vector]
cars_train_features &lt;- cars[,-1]</code></pre>
</div>
<div id="removing-problematic-features" class="section level1">
<h1>4 Removing problematic features</h1>
<p>Highly correlated variables can influence the prediction of a linear model. From this point of view, it is recommended to overcome this in the first step and to exclude problematic features.</p>
</div>
<div id="machine-dataset" class="section level1">
<h1>4.1 Machine dataset</h1>
<pre class="r"><code>set.seed(4352345)

machine_correlations &lt;- cor(machine_train_features)
findCorrelation(machine_correlations)</code></pre>
<pre><code>## integer(0)</code></pre>
<p>As we can see, there is no correlation above 0.9 (by default). Let’s check correlations of 0.75 by this.</p>
<pre class="r"><code>findCorrelation(machine_correlations, cutoff = 0.75)</code></pre>
<pre><code>## [1] 3</code></pre>
<pre class="r"><code>cor(machine_train$MMIN, machine_train$MMAX)</code></pre>
<pre><code>## [1] 0.7588448</code></pre>
<p>Ok we have three of them but 0.75 should not be problematic.</p>
<pre class="r"><code>findLinearCombos(machine_correlations)</code></pre>
<pre><code>## $linearCombos
## list()
## 
## $remove
## NULL</code></pre>
</div>
<div id="cars-dataset" class="section level1">
<h1>4.2 Cars dataset</h1>
<p>Let’s have a look at the cars dataset.</p>
<pre class="r"><code>set.seed(232455)
cars_cor &lt;- cor(cars_train_features)
findCorrelation(cars_cor)</code></pre>
<pre><code>## integer(0)</code></pre>
<pre class="r"><code>findCorrelation(cars_cor, cutoff = 0.75)</code></pre>
<pre><code>## [1] 3</code></pre>
<p>Again three correlations between 0.75 and 0.9 (by default). One example of these three are the correlation betwenn the variables <em>Doors</em> and <em>Coupe</em>.</p>
<pre class="r"><code>cor(cars$Doors,cars$coupe)</code></pre>
<pre><code>## [1] -0.8254435</code></pre>
<pre class="r"><code>table(cars$coupe,cars$Doors)</code></pre>
<pre><code>##    
##       2   4
##   0  50 614
##   1 140   0</code></pre>
<p>Another usefull function is the <em>findLinearCombos</em> function to detect exact linear combinations of other features.</p>
<pre class="r"><code>findLinearCombos(cars)</code></pre>
<pre><code>## $linearCombos
## $linearCombos[[1]]
## [1] 15  4  8  9 10 11 12 13 14
## 
## $linearCombos[[2]]
##  [1] 18  4  8  9 10 11 12 13 16 17
## 
## 
## $remove
## [1] 15 18</code></pre>
<p>Here we are advised to drop the <em>coupe</em> and <em>wagon</em> columns, which are the 15th and 18th features, respectively, because they are exact linear combinations of other features. Note that the division into a training part and a test part must be redone.</p>
<pre class="r"><code>set.seed(232555)
cars &lt;- cars[,c(-15, -18)]
cars_sampling_vector &lt;- createDataPartition(cars$Price, p = 0.85, list = FALSE)
cars_train &lt;- cars[cars_sampling_vector,]
cars_train_labels &lt;- cars$Price[cars_sampling_vector]
cars_test &lt;- cars[-cars_sampling_vector,]
cars_test_labels &lt;- cars$Price[-cars_sampling_vector]
cars_train_features &lt;- cars[,-1]</code></pre>
</div>
<div id="assessing-linear-regression-models" class="section level1">
<h1>5 Assessing linear regression models</h1>
<p>Once all the problematic features have been identified and excluded, it is time to create the regression models.</p>
<pre class="r"><code>machine_model1 &lt;- lm(PRP ~ ., data = machine_train)
cars_model1 &lt;- lm(Price ~ ., data = cars_train)</code></pre>
<p>Here the summary for the machine_model1:</p>
<pre class="r"><code>summary(machine_model1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = PRP ~ ., data = machine_train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -209.14  -27.82    7.48   28.70  361.38 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -6.092e+01  8.772e+00  -6.945 7.47e-11 ***
## MYCT         5.394e-02  1.908e-02   2.827 0.005253 ** 
## MMIN         1.586e-02  1.965e-03   8.072 1.15e-13 ***
## MMAX         5.998e-03  6.948e-04   8.633 3.92e-15 ***
## CACH         5.143e-01  1.500e-01   3.428 0.000761 ***
## CHMIN       -5.791e-01  8.967e-01  -0.646 0.519230    
## CHMAX        1.581e+00  2.310e-01   6.845 1.29e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 61.28 on 172 degrees of freedom
## Multiple R-squared:  0.8728, Adjusted R-squared:  0.8684 
## F-statistic: 196.7 on 6 and 172 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>And here the summary for the cars_model1:</p>
<pre class="r"><code>summary(cars_model1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Price ~ ., data = cars_train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -9775.1 -1480.2   116.4  1516.7 12931.9 
## 
## Coefficients: (1 not defined because of singularities)
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -1.195e+03  1.086e+03  -1.100  0.27182    
## Mileage     -1.902e-01  1.395e-02 -13.641  &lt; 2e-16 ***
## Cylinder     3.668e+03  1.243e+02  29.504  &lt; 2e-16 ***
## Doors        1.643e+03  2.803e+02   5.862  7.2e-09 ***
## Cruise       2.233e+02  3.253e+02   0.686  0.49271    
## Sound        5.008e+02  2.571e+02   1.948  0.05184 .  
## Leather      7.564e+02  2.713e+02   2.788  0.00545 ** 
## Buick        1.141e+03  6.036e+02   1.890  0.05924 .  
## Cadillac     1.369e+04  6.842e+02  20.003  &lt; 2e-16 ***
## Chevy       -4.845e+02  4.877e+02  -0.993  0.32083    
## Pontiac     -1.395e+03  5.378e+02  -2.594  0.00971 ** 
## Saab         1.230e+04  6.143e+02  20.031  &lt; 2e-16 ***
## Saturn              NA         NA      NA       NA    
## convertible  1.133e+04  5.892e+02  19.232  &lt; 2e-16 ***
## hatchback   -6.586e+03  6.587e+02  -9.998  &lt; 2e-16 ***
## sedan       -4.583e+03  4.774e+02  -9.600  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2912 on 669 degrees of freedom
## Multiple R-squared:  0.9165, Adjusted R-squared:  0.9147 
## F-statistic: 524.2 on 14 and 669 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Please note the following message from the output: “Coefficients: (1 not defined because of singularities)”. This occurs because we still have features whose effect on the output is indiscernible form other features due to underlying dependencies. We can identify this variable using the <em>alias</em> function.</p>
<pre class="r"><code>alias(cars_model1)</code></pre>
<pre><code>## Model :
## Price ~ Mileage + Cylinder + Doors + Cruise + Sound + Leather + 
##     Buick + Cadillac + Chevy + Pontiac + Saab + Saturn + convertible + 
##     hatchback + sedan
## 
## Complete :
##        (Intercept) Mileage Cylinder Doors Cruise Sound Leather Buick
## Saturn  1           0       0        0     0      0     0      -1   
##        Cadillac Chevy Pontiac Saab convertible hatchback sedan
## Saturn -1       -1    -1      -1    0           0         0</code></pre>
<p>Now we know, that we have to remove the <em>Saturn</em> variable.</p>
<pre class="r"><code>cars_model2 &lt;- lm(Price ~. -Saturn, data = cars_train)
summary(cars_model2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Price ~ . - Saturn, data = cars_train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -9775.1 -1480.2   116.4  1516.7 12931.9 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -1.195e+03  1.086e+03  -1.100  0.27182    
## Mileage     -1.902e-01  1.395e-02 -13.641  &lt; 2e-16 ***
## Cylinder     3.668e+03  1.243e+02  29.504  &lt; 2e-16 ***
## Doors        1.643e+03  2.803e+02   5.862  7.2e-09 ***
## Cruise       2.233e+02  3.253e+02   0.686  0.49271    
## Sound        5.008e+02  2.571e+02   1.948  0.05184 .  
## Leather      7.564e+02  2.713e+02   2.788  0.00545 ** 
## Buick        1.141e+03  6.036e+02   1.890  0.05924 .  
## Cadillac     1.369e+04  6.842e+02  20.003  &lt; 2e-16 ***
## Chevy       -4.845e+02  4.877e+02  -0.993  0.32083    
## Pontiac     -1.395e+03  5.378e+02  -2.594  0.00971 ** 
## Saab         1.230e+04  6.143e+02  20.031  &lt; 2e-16 ***
## convertible  1.133e+04  5.892e+02  19.232  &lt; 2e-16 ***
## hatchback   -6.586e+03  6.587e+02  -9.998  &lt; 2e-16 ***
## sedan       -4.583e+03  4.774e+02  -9.600  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2912 on 669 degrees of freedom
## Multiple R-squared:  0.9165, Adjusted R-squared:  0.9147 
## F-statistic: 524.2 on 14 and 669 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Perfect!</p>
</div>
<div id="how-to-check-some-summary-outputs-individually" class="section level1">
<h1>6 How to check some summary outputs individually</h1>
<p>All the information which we receive with the summary command can also be calculated automatically:</p>
</div>
<div id="residual-analysis" class="section level1">
<h1>6.1 Residual analysis</h1>
<p>A residual is simply the error our model makes for a paritcular observation.</p>
<pre class="r"><code>summary(cars_model2$residuals)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -9775.1 -1480.2   116.4     0.0  1516.7 12931.9</code></pre>
<pre class="r"><code>mean(cars_train$Price)</code></pre>
<pre><code>## [1] 21366.73</code></pre>
<p>With this information we can say that the average selling price of a car in our training data is around 21k Dollar, and 50% of our predictions are roughly within +- 1.6k of the correct value.</p>
<p>A useful way to graphically compare the quantiles of the distributions of two quantitative variables is the quantile-quantile diagram.</p>
<pre class="r"><code>par(mfrow = c(2, 1))
machine_residuals &lt;- machine_model1$residuals
qqnorm(machine_residuals, main = &quot;Normal Q-Q Plot for CPU data set&quot;)
qqline(machine_residuals)
cars_residuals &lt;- cars_model2$residuals
qqnorm(cars_residuals, main = &quot;Normal Q-Q Plot for Cars data set&quot;)
qqline(cars_residuals)</code></pre>
<p><img src="/post/2019-01-11-machine-learning-predictions-with-linear-regressions_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>As we can see the residuals from both models seem to lie reasonably close to the theoretical quantiles of a normal distribution, although the fit isn’t perfect, as is typical with most real-world data.</p>
</div>
<div id="significance-tests-for-linear-regression" class="section level1">
<h1>6.2 Significance tests for linear regression</h1>
<p>Let’s have a look at the machine_model1 output again:</p>
<pre class="r"><code>summary(machine_model1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = PRP ~ ., data = machine_train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -209.14  -27.82    7.48   28.70  361.38 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -6.092e+01  8.772e+00  -6.945 7.47e-11 ***
## MYCT         5.394e-02  1.908e-02   2.827 0.005253 ** 
## MMIN         1.586e-02  1.965e-03   8.072 1.15e-13 ***
## MMAX         5.998e-03  6.948e-04   8.633 3.92e-15 ***
## CACH         5.143e-01  1.500e-01   3.428 0.000761 ***
## CHMIN       -5.791e-01  8.967e-01  -0.646 0.519230    
## CHMAX        1.581e+00  2.310e-01   6.845 1.29e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 61.28 on 172 degrees of freedom
## Multiple R-squared:  0.8728, Adjusted R-squared:  0.8684 
## F-statistic: 196.7 on 6 and 172 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>We can compute some figures manually, eg. the t-value and p-value of the MYCT variable:</p>
<pre class="r"><code>q &lt;- 5.210e-02 / 1.885e-02
q</code></pre>
<pre><code>## [1] 2.763926</code></pre>
<pre class="r"><code>pt(q, df = 172, lower.tail = F) * 2</code></pre>
<pre><code>## [1] 0.006333496</code></pre>
</div>
<div id="residual-standard-error-rse" class="section level1">
<h1>6.3 Residual Standard Error (RSE)</h1>
<p>We define a metric known as the Residual Standard Error, which estimates the standard deviation of our model compared to the target function.</p>
<p>We can compute the RSE for our two models using the preceding formular as follows:</p>
<pre class="r"><code>n_machine &lt;- nrow(machine_train)
k_machine &lt;- length(machine_model1$coefficients) - 1
sqrt(sum(machine_model1$residuals ^ 2) / (n_machine - k_machine - 1))</code></pre>
<pre><code>## [1] 61.27671</code></pre>
<pre class="r"><code>n_cars &lt;- nrow(cars_train)
k_cars &lt;- length(cars_model2$coefficients) - 1
sqrt(sum(cars_model2$residuals ^ 2) / (n_cars - k_cars - 1))</code></pre>
<pre><code>## [1] 2912.322</code></pre>
<p>To interpret the RSE values for our two models, we neet to compare them with the mean of our output variables.</p>
<pre class="r"><code>mean(machine_train$PRP)</code></pre>
<pre><code>## [1] 108.2235</code></pre>
<pre class="r"><code>mean(cars_train$Price)</code></pre>
<pre><code>## [1] 21366.73</code></pre>
<p>We can see:</p>
<ul>
<li>RSE machine_train: 60,95 and mean 102,51</li>
<li>RSE cars_train: 2.946,98 and mean 2.132,20</li>
</ul>
</div>
<div id="r2" class="section level1">
<h1>6.4 R<sup>2</sup></h1>
<p>Now let’s compute R<sup>2</sup> manually to compare different regression models.</p>
<pre class="r"><code>compute_rsquared &lt;- function(x, y) {
  rss &lt;- sum((x - y) ^ 2)
  tss &lt;- sum((y - mean(y)) ^ 2)
  return(1 - (rss / tss))
}

compute_rsquared(machine_model1$fitted.values, machine_train$PRP)</code></pre>
<pre><code>## [1] 0.872797</code></pre>
<pre class="r"><code>compute_rsquared(cars_model2$fitted.values, cars_train$Price)</code></pre>
<pre><code>## [1] 0.9164532</code></pre>
</div>
<div id="adjusted-r2" class="section level1">
<h1>6.5 Adjusted R<sup>2</sup></h1>
<p>We can also do the same manual calculation for the adjusted R<sup>2</sup>.</p>
<pre class="r"><code>compute_adjusted_rsquared &lt;- function(x, y, k) {
  n &lt;- length(y)
  r2 &lt;- compute_rsquared(x, y)
  return(1 - ((1 - r2) * (n - 1) / (n - k - 1)))
}

compute_adjusted_rsquared(machine_model1$fitted.values, machine_train$PRP, k_machine)</code></pre>
<pre><code>## [1] 0.8683597</code></pre>
<pre class="r"><code>compute_adjusted_rsquared(cars_model2$fitted.values, cars_train$Price, k_cars)</code></pre>
<pre><code>## [1] 0.9147048</code></pre>
</div>
<div id="test-set-performance-with-the-mse" class="section level1">
<h1>7 Test set performance with the MSE</h1>
<p>Now it’s time to check the test set performance of our models. We can do this by the Mean Squared Error (<strong>MSE</strong>).</p>
<pre class="r"><code>machine_model1_predictions &lt;- predict(machine_model1, machine_test)
cars_model2_predictions &lt;- predict(cars_model2, cars_test)</code></pre>
<p>Here is the required function for it:</p>
<pre class="r"><code>compute_mse &lt;- function(predictions, actual) { 
  mean( (predictions - actual) ^ 2 ) 
}</code></pre>
</div>
<div id="mse-for-the-machine-dataset" class="section level1">
<h1>7.1 MSE for the machine dataset</h1>
<p>Now we can compare the training and the test MSE for both models</p>
<pre class="r"><code>compute_mse(machine_model1$fitted.values, machine_train$PRP)</code></pre>
<pre><code>## [1] 3607.998</code></pre>
<pre class="r"><code>compute_mse(machine_model1_predictions, machine_test$PRP)</code></pre>
<pre><code>## [1] 2961.236</code></pre>
</div>
<div id="mse-for-the-cars-dataset" class="section level1">
<h1>7.2 MSE for the cars dataset</h1>
<pre class="r"><code>compute_mse(cars_model2$fitted.values, cars_train$Price)</code></pre>
<pre><code>## [1] 8295616</code></pre>
<pre class="r"><code>compute_mse(cars_model2_predictions, cars_test$Price)</code></pre>
<pre><code>## [1] 8386007</code></pre>
</div>
<div id="problems-with-linear-regression" class="section level1">
<h1>8 Problems with linear regression</h1>
<p>In my post <a href="https://michael-fuchs.netlify.com/2018/09/21/regression-analysis/">“Regression Analysis”</a> paragraph 5 Model assumption I have already addressed several aspects that must be taken into account when creating a regression model. For the present investigation of the two datasets two aspects are dealt with in detail.</p>
</div>
<div id="multicollinearity" class="section level1">
<h1>8.1 Multicollinearity</h1>
<p>Multicollinearity is a problem of regression analysis and occurs when two or more explanatory variables have a very strong correlation with each other. We can check this with the <em>vif</em> function.</p>
<pre class="r"><code>vif(cars_model2)</code></pre>
<pre><code>##     Mileage    Cylinder       Doors      Cruise       Sound     Leather 
##    1.012546    2.408258    4.502339    1.574441    1.154645    1.209674 
##       Buick    Cadillac       Chevy     Pontiac        Saab convertible 
##    2.698820    3.335587    4.621721    3.504974    3.639406    1.613480 
##   hatchback       sedan 
##    2.414707    4.373753</code></pre>
<p>These values can also be calculated manually by us. Here is the example for the calculation of the value for the variable Sedan</p>
<pre class="r"><code>sedan_model &lt;- lm(sedan ~ .-Price -Saturn, data = cars_train)
sedan_r2 &lt;- compute_rsquared(sedan_model$fitted.values, cars_train$sedan)
1 / (1-sedan_r2)</code></pre>
<pre><code>## [1] 4.373753</code></pre>
</div>
<div id="outliers" class="section level1">
<h1>8.2 Outliers</h1>
<p>Outliers should always be taken into account in predictive models, as they can greatly influence the forecast, as the following example shows:</p>
<pre class="r"><code>scatterplot(machine_model1$fitted.values, machine_model1$residuals, id = TRUE, xlab = &quot;Fitted Values&quot;, ylab = &quot;Residuals&quot;, main = &quot;Searching for outliers&quot;)</code></pre>
<p><img src="/post/2019-01-11-machine-learning-predictions-with-linear-regressions_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
<pre><code>## [1]   8 172</code></pre>
<p>As we can see in the graphic line 172 is an outlier. Now we check what effect it has when we exclude line 172 from the training part.</p>
<pre class="r"><code>machine_model2 &lt;- lm(PRP ~ ., data = machine_train[!(rownames(machine_train)) %in% c(171),])

summary(machine_model2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = PRP ~ ., data = machine_train[!(rownames(machine_train)) %in% 
##     c(171), ])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -179.29  -26.08    7.01   25.85  418.17 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -5.556e+01  8.809e+00  -6.307 2.35e-09 ***
## MYCT         5.026e-02  1.875e-02   2.680  0.00807 ** 
## MMIN         1.634e-02  1.934e-03   8.449 1.23e-14 ***
## MMAX         5.496e-03  7.041e-04   7.805 5.67e-13 ***
## CACH         5.375e-01  1.473e-01   3.648  0.00035 ***
## CHMIN        1.896e-02  9.044e-01   0.021  0.98330    
## CHMAX        1.332e+00  2.432e-01   5.477 1.53e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 60.08 on 171 degrees of freedom
## Multiple R-squared:  0.8604, Adjusted R-squared:  0.8555 
## F-statistic: 175.7 on 6 and 171 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div id="compare-rse" class="section level1">
<h1>8.2.1 Compare RSE</h1>
<pre class="r"><code>sqrt(sum(machine_model1$residuals ^ 2) / (n_machine - k_machine - 1))</code></pre>
<pre><code>## [1] 61.27671</code></pre>
<pre class="r"><code>k_machine &lt;- length(machine_model2$coefficients) - 1
sqrt(sum(machine_model2$residuals ^ 2) / (n_machine - k_machine - 1))</code></pre>
<pre><code>## [1] 59.90352</code></pre>
<p>Here we cas see the imporvement of RSE:</p>
<ul>
<li>machine_model1: 58,43</li>
<li>machine_model2: 57,81</li>
</ul>
</div>
<div id="compare-mse" class="section level1">
<h1>8.2.2 Compare MSE</h1>
<pre class="r"><code>compute_mse(machine_model1_predictions, machine_test$PRP)</code></pre>
<pre><code>## [1] 2961.236</code></pre>
<pre class="r"><code>machine_model2_predictions &lt;- predict(machine_model2, machine_test)
compute_mse(machine_model2_predictions, machine_test$PRP)</code></pre>
<pre><code>## [1] 2570.677</code></pre>
<p>Here we cas see the imporvement of MSE:</p>
<ul>
<li>machine_model1: 6.191,99</li>
<li>machine_model2: 5.197,30</li>
</ul>
</div>
<div id="future-selection" class="section level1">
<h1>9 Future selection</h1>
<p>Future selection is a machine learning approach, that uses only a subset of the available features for a learning algorithm. To do this we need the null-modell of both regression models.</p>
<pre class="r"><code>machine_model_null &lt;- lm(PRP ~ 1, data = machine_train[!(rownames(machine_train)) %in% c(171),])
cars_model_null &lt;- lm(Price ~ 1, data = cars_train)</code></pre>
<p>There are two types of future selection to operate: the forward selection and the backward selection (both demonstrated below). Note that the lower the AIC, the better the model.</p>
</div>
<div id="forward-selection" class="section level1">
<h1>9.1 Forward selection</h1>
<pre class="r"><code>machine_model3 &lt;- step(machine_model_null, scope = list(lower = machine_model_null, upper = machine_model2), direction = &quot;forward&quot;)</code></pre>
<pre><code>## Start:  AIC=1803.44
## PRP ~ 1
## 
##         Df Sum of Sq     RSS    AIC
## + MMAX   1   3246462 1176168 1569.7
## + MMIN   1   2971611 1451019 1607.1
## + CACH   1   1911921 2510709 1704.7
## + CHMIN  1   1684978 2737652 1720.1
## + CHMAX  1   1291177 3131453 1744.0
## + MYCT   1    406943 4015687 1788.3
## &lt;none&gt;               4422630 1803.4
## 
## Step:  AIC=1569.69
## PRP ~ MMAX
## 
##         Df Sum of Sq     RSS    AIC
## + MMIN   1    272414  903755 1524.8
## + CACH   1    234641  941527 1532.1
## + CHMAX  1    127231 1048938 1551.3
## + CHMIN  1     95916 1080253 1556.5
## &lt;none&gt;               1176168 1569.7
## + MYCT   1      3543 1172625 1571.2
## 
## Step:  AIC=1524.79
## PRP ~ MMAX + MMIN
## 
##         Df Sum of Sq    RSS    AIC
## + CHMAX  1    215262 688493 1478.4
## + CACH   1    141593 762162 1496.5
## + CHMIN  1     65723 838032 1513.3
## &lt;none&gt;               903755 1524.8
## + MYCT   1      8549 895205 1525.1
## 
## Step:  AIC=1478.36
## PRP ~ MMAX + MMIN + CHMAX
## 
##         Df Sum of Sq    RSS    AIC
## + CACH   1     45317 643175 1468.2
## + MYCT   1     20176 668316 1475.1
## &lt;none&gt;               688493 1478.4
## + CHMIN  1      2150 686343 1479.8
## 
## Step:  AIC=1468.25
## PRP ~ MMAX + MMIN + CHMAX + CACH
## 
##         Df Sum of Sq    RSS    AIC
## + MYCT   1   25963.4 617212 1462.9
## &lt;none&gt;               643175 1468.2
## + CHMIN  1      31.8 643143 1470.2
## 
## Step:  AIC=1462.91
## PRP ~ MMAX + MMIN + CHMAX + CACH + MYCT
## 
##         Df Sum of Sq    RSS    AIC
## &lt;none&gt;               617212 1462.9
## + CHMIN  1    1.5868 617210 1464.9</code></pre>
</div>
<div id="backward-selection" class="section level1">
<h1>9.2 Backward selection</h1>
<pre class="r"><code>cars_model3 &lt;- step(cars_model2, scope = list(lower=cars_model_null, upper=cars_model2), direction = &quot;backward&quot;)</code></pre>
<pre><code>## Start:  AIC=10926.97
## Price ~ (Mileage + Cylinder + Doors + Cruise + Sound + Leather + 
##     Buick + Cadillac + Chevy + Pontiac + Saab + Saturn + convertible + 
##     hatchback + sedan) - Saturn
## 
##               Df  Sum of Sq        RSS   AIC
## - Cruise       1    3995915 5.6782e+09 10925
## - Chevy        1    8371573 5.6826e+09 10926
## &lt;none&gt;                      5.6742e+09 10927
## - Buick        1   30285795 5.7045e+09 10929
## - Sound        1   32182763 5.7064e+09 10929
## - Pontiac      1   57052198 5.7313e+09 10932
## - Leather      1   65930873 5.7401e+09 10933
## - Doors        1  291416934 5.9656e+09 10959
## - sedan        1  781683537 6.4559e+09 11013
## - hatchback    1  847888500 6.5221e+09 11020
## - Mileage      1 1578297060 7.2525e+09 11093
## - convertible  1 3137062272 8.8113e+09 11226
## - Cadillac     1 3393682599 9.0679e+09 11246
## - Saab         1 3403047814 9.0772e+09 11246
## - Cylinder     1 7383246635 1.3057e+10 11495
## 
## Step:  AIC=10925.45
## Price ~ Mileage + Cylinder + Doors + Sound + Leather + Buick + 
##     Cadillac + Chevy + Pontiac + Saab + convertible + hatchback + 
##     sedan
## 
##               Df  Sum of Sq        RSS   AIC
## - Chevy        1    7186183 5.6854e+09 10924
## &lt;none&gt;                      5.6782e+09 10925
## - Sound        1   32029542 5.7102e+09 10927
## - Buick        1   35991456 5.7142e+09 10928
## - Pontiac      1   54308001 5.7325e+09 10930
## - Leather      1   63344670 5.7415e+09 10931
## - Doors        1  287422460 5.9656e+09 10957
## - sedan        1  777758351 6.4560e+09 11011
## - hatchback    1  858285947 6.5365e+09 11020
## - Mileage      1 1575276046 7.2535e+09 11091
## - convertible  1 3139915902 8.8181e+09 11224
## - Cadillac     1 3445748225 9.1239e+09 11248
## - Saab         1 4099244521 9.7774e+09 11295
## - Cylinder     1 8763979648 1.4442e+10 11562
## 
## Step:  AIC=10924.31
## Price ~ Mileage + Cylinder + Doors + Sound + Leather + Buick + 
##     Cadillac + Pontiac + Saab + convertible + hatchback + sedan
## 
##               Df  Sum of Sq        RSS   AIC
## &lt;none&gt;                      5.6854e+09 10924
## - Sound        1   27117903 5.7125e+09 10926
## - Leather      1   58342039 5.7437e+09 10929
## - Pontiac      1   66094962 5.7515e+09 10930
## - Buick        1  112035733 5.7974e+09 10936
## - Doors        1  283926365 5.9693e+09 10956
## - sedan        1  771530952 6.4569e+09 11009
## - hatchback    1  874783931 6.5602e+09 11020
## - Mileage      1 1571195308 7.2566e+09 11089
## - convertible  1 3135654516 8.8210e+09 11223
## - Cadillac     1 6341658279 1.2027e+10 11435
## - Saab         1 8000364498 1.3686e+10 11523
## - Cylinder     1 8891934347 1.4577e+10 11566</code></pre>
</div>
<div id="comparing-the-calculated-mse" class="section level1">
<h1>9.3 Comparing the calculated MSE</h1>
<p>Now we can again calculate the MSE for the best models selected by the feature selection:</p>
<pre class="r"><code>machine_model3_predictions &lt;- predict(machine_model3, machine_test)
compute_mse(machine_model3_predictions, machine_test$PRP)</code></pre>
<pre><code>## [1] 2572.768</code></pre>
<pre class="r"><code>cars_model3_predictions &lt;- predict(cars_model3, cars_test)
compute_mse(cars_model3_predictions, cars_test$Price)</code></pre>
<pre><code>## [1] 8465003</code></pre>
<p>Let’s compare the results with the previous models:</p>
<p><strong>Machine dataset:</strong></p>
<ul>
<li>MSE machine_model1_predictions: 6.191,99</li>
<li>MSE machine_model2_predictions: 5.197,30</li>
<li>MSE machine_model3_predictions: 5.057,91</li>
</ul>
<p><strong>Cars dataset:</strong></p>
<ul>
<li>MSE cars_model2_predictions: 7.180.150</li>
<li>MSE cars_model3_predictions: 7.262.383</li>
</ul>
</div>
<div id="regularization" class="section level1">
<h1>10 Regularization</h1>
<p>In addition to the future selection just shown, a regularization can also be used. In the following two methods will be shown on the basis of the car dataset: the ridge method and the lasso method.</p>
<pre class="r"><code>cars_train_mat &lt;- model.matrix(Price ~ .-Saturn, cars_train)[,-1]
lambdas &lt;- 10 ^ seq(8, -4, length = 250)
cars_models_ridge &lt;- glmnet(cars_train_mat, cars_train$Price, alpha = 0, lambda = lambdas)
cars_models_lasso &lt;- glmnet(cars_train_mat, cars_train$Price, alpha = 1, lambda = lambdas)</code></pre>
<p>Here we can see how the Ridge Regression and the Lasso method works:</p>
<pre class="r"><code>layout(matrix(c(1, 2), 1, 2))
plot(cars_models_ridge, xvar = &quot;lambda&quot;, main = &quot;Ridge Regression&quot;)
plot(cars_models_lasso, xvar = &quot;lambda&quot;, main = &quot;Lasso&quot;)</code></pre>
<p><img src="/post/2019-01-11-machine-learning-predictions-with-linear-regressions_files/figure-html/unnamed-chunk-51-1.png" width="672" /></p>
<p>Here is a calculation to determine the best lambda for the two methods:</p>
<pre class="r"><code>ridge.cv &lt;- cv.glmnet(cars_train_mat, cars_train$Price, alpha = 0)
lambda_ridge &lt;- ridge.cv$lambda.min
lambda_ridge</code></pre>
<pre><code>## [1] 730.9619</code></pre>
<pre class="r"><code>lasso.cv &lt;- cv.glmnet(cars_train_mat, cars_train$Price, alpha = 1)
lambda_lasso &lt;- lasso.cv$lambda.min
lambda_lasso</code></pre>
<pre><code>## [1] 10.85455</code></pre>
<p>Again we’ll compare the new MSE:</p>
<pre class="r"><code>cars_test_mat &lt;- model.matrix(Price ~ . -Saturn, cars_test)[,-1]
cars_ridge_predictions &lt;- predict(cars_models_ridge, s = lambda_ridge, newx = cars_test_mat)
compute_mse(cars_ridge_predictions, cars_test$Price)</code></pre>
<pre><code>## [1] 8318101</code></pre>
<pre class="r"><code>cars_lasso_predictions &lt;- predict(cars_models_lasso, s = lambda_lasso, newx = cars_test_mat)
compute_mse(cars_lasso_predictions, cars_test$Price)</code></pre>
<pre><code>## [1] 8330878</code></pre>
<p>The lasso-model performs best. Here again an overview of the different cars model prediction MSEs:</p>
<ul>
<li>MSE cars_model2_predictions: 7.180.150</li>
<li>MSE cars_model3_predictions: 7.262.383</li>
<li>MSE cars_model_lasso_predictions: 7.173.818</li>
</ul>
</div>
<div id="conclusion" class="section level1">
<h1>11 Conclusion</h1>
<p>We have seen how to significantly improve the performance of linear models.</p>
<p>Machine_model3_predictions with MSE 5.057,91 proved to be the best for the machine data set. We achieved this result with the forward selection.</p>
<p>In contrast to that cars_model_lasso_predictions with MSE 7.173.818 proved to be the best for the cars data set.</p>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdn.bootcss.com/highlight.js/9.11.0/highlight.min.js"></script>



<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/r.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

